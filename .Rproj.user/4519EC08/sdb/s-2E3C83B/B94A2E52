{
    "contents" : "#' A Text Loading Function\n#'\n#' This function chews a load of pdfs in a fixed directory\n#' @keywords impact ref text\n#' @export\n#' @examples\n#' allTogether(dirName={some directory string})\nallTogether <- function (dirName=\"C:/Martin/Data/Impact/REF submission documents/Templates\") {\n  docs <- loadDocs (dirName)\n  postDocLoad (docs)\n}\n\n\n\npostDocLoad = function (docs) {\n  library(stringr)\n  library(tm)\n  \n  # get docs and impscores\n  vals <- attachMetaData (docs)\n  docs <- vals$docs\n  impScores <- vals$impScores\n  \n  # make a colour scale based on impact scores\n  tcol <- makeColourScale (docs, impScores)\n  \n  # do readability scores before punctuation is stripped out\n  readScores <- readability (docs)\n  levels <- lapply (readScores, function(i) i$Readability$FK_read.ease)\n  drawWeightGPAPlot (docs, impScores, levels, tcol)\n  \n  \n  # strip out punctuation, whitespace, common words, do stemming etc\n  docs <- filterOutOwnName (docs)\n  docs <- docProcessing (docs, c(\"impact\"))\n  \n  # search for years, strip numbers out of docs afterwards\n  yrange <- extractYears (docs)\n  docs <- tm_map (docs, removeNumbers)\n  \n  sect <- cropDocs (docs, startCut=\"context\", stopCut=NA)\n  matrices <- makeTermMatrices (sect, c(2, nrows(docs) * 0.8))\n  \n  \n  results <- drawPlots (goodSect, matrices$inverse, impScores, tcol)\n  wscores <- wordScoresImpactWeighted (docs, impScores)\n  \n  topicModel <- makeTopicModel (matrices$direct)\n  tscores <- lda@gamma\n  tindices <- sapply(seq(1,nrow(tscores)), function(i) which(tscores[i,] == max(tscores[i,])))\n  drawWeightGPAPlot (docs, impScores, tindices, tcol)\n  \n  topicModel2 <- makeSupervisedTopicModel (matrices$direct)\n  tscores <- lda@gamma\n  tindices <- sapply(seq(1,nrow(tscores)), function(i) which(tscores[i,] == max(tscores[i,])))\n  drawWeightGPAPlot (docs, impScores, tindices, tcol)\n}\n\n attachMetaData = function (docs) {\n   # read in impact scores, get institute names from filenames, keep only institutions that have impact scores\n   impScores <- impactScoreReader ()\n   multVals <- institutesFromFilenames (docs)\n\n   docs <- multVals$docs\n   docs <- filterOutNA (docs, impScores)\n   # make impscores index match docs index\n   impScores <- subset (impScores, !is.na(GPA))\n   \n   list(docs=docs, uniNames=multVals$uniNames, impScores = impScores)\n }\n\n\nloadDocs <- function (dirName=\"C:/Martin/Data/Impact/REF submission documents/Templates\") {\n  library(\"RWeka\")\n  library(tm)\n  library(stringr)\n  \n  dirPath = file.path(dirName)\n  docs = Corpus (DirSource (dirPath), readerControl = list(reader=readPDF))\n  \n  docs\n}\n\ndocProcessing <- function (docs, myStops = c()) \n{\n  library(tm)\n  library(SnowballC)\n  \n  # strip simple words, punctuation, whitespace etc\n  docs = tm_map (docs, content_transformer (tolower))\n  docs = tm_map (docs, removeWords, stopwords(\"english\"))\n  docs = tm_map (docs, removeWords, myStops)\n  docs = tm_map (docs, stripWhitespace)\n  docs = tm_map (docs, removePunctuation)\n  #docs = tm_map (docs, removeNumbers)\n  docs = tm_map (docs, removeWords, myStops)\n  \n  # stem document, so globalise, globally both --> global\n  docs = tm_map (docs, stemDocument, lazy=TRUE)\n  \n  docs\n}\n\n\n\ninstitutesFromFilenames <- function (docs) {\n  fileNames <- names (docs)\n  uninames <- as.vector (sapply (fileNames, function(x) substr (x, 1, str_locate (x, \" - 19 .pdf\") - 1)))\n  print (uninames)\n  \n  # put the names back in individually into the metadata as well\n  for (i in seq_along(docs)) { meta(docs[[i]])$name <- uninames[i] }\n  \n  # multiple values returned\n  # need to return docs as R doesn't pass in docs by reference\n  # instead docs is copied by value so any changes I make here don't automatically\n  # happen to 'docs' outside this function\n  \n  list('docs'=docs, 'uninames'=uninames)\n}\n\n\n# Strip universities titles' out of documents\nfilterOutOwnName <- function (docs) {\n  perDoc <- function (doc) {\n    name <- doc$meta$name\n    doc$content <- gsub (name, \"\", doc$content)\n    \n    doc\n  }\n  \n  docs$content <- lapply (docs, perDoc)\n  \n  docs\n}\n\n\nimpactScoreReader <- function (cname=\"C:/Martin/Data/Impact/REF2014-Results-xlsx.csv\") {\n  library (data.table)\n  \n  csv <- read.csv (cname)\n  csv2 <- subset(csv, Profile==\"Impact\")\n  csv3 <- subset(csv2, Unit.of.assessment.number==19)\n  \n  # trying to figure out how to do stuff to  a list, csv4 not working\n  \n  gpacalc <- function (x) { ((as.numeric(x['X4.']) * 4) + (as.numeric(x['X3.']) * 3)  \n                            + (as.numeric(x['X2.']) * 2) + (as.numeric(x['X1.']) * 1)) / 100.0\n  }\n  gpa <- apply (csv3, 1, gpacalc)\n  names <- as.character (csv3$Institution.name)\n  sFrame <- data.frame (names, gpa);\n  names(sFrame) <- c(\"Name\", \"GPA\")\n  \n  sTable <- data.table (sFrame, key=c('Name'))\n  sTable\n}\n\ncropDocs <- function (docs, startCut=\"context\", stopCut=NA, inclusive=FALSE) {\n  \n  cropper <- function (doc) {\n\n    d <- content(doc)\n    end <- length(d)\n\n    if (end > 10) {\n      \n      start <- 1\n      \n      if (!is.na(stopCut)) {\n        for (j in end:1) {\n          rf <- str_locate (d[j], stopCut)[1]\n          if (!is.na(rf)) {\n            end <- j - (if (inclusive) 1 else 0)\n            break\n          }\n        }\n      }\n      \n      if (!is.na(startCut)) {\n        for (j in 1:end) {\n          rf <- str_locate (d[j], startCut)[1]\n          if (!is.na(rf)) {\n            start <- j + (if (inclusive) 0 else 1)\n            break\n          }\n        }\n      }\n      \n      content(doc) <- d[start:end]\n      doc\n    }\n  }\n  \n  docs$content <- lapply (docs, cropper)\n  \n  ### filter out near empty text strings (4 chars or less)\n  small <- function (doc) {\n    con <- content(doc)\n    content(doc) <- con[nchar(con) > 4]\n    doc\n  }\n  docs$content <- lapply (docs, small)\n  \n  \n  docs\n}\n\nreadability <- function (docs) {\n  library (qdap)\n  library (munsell)\n  \n  # qdapify\n  qdocs <- as.data.frame (docs, col1=\"docs\", col2=\"text\")\n  \n  # Split document text strings up into sentences\n  qsplitdocs <- sentSplit (qdocs, \"text\")\n  # now lots of fragments, mapped by filename\n  # gather fragments up by same filename and stick them through FK test\n  perFile <- unique (qsplitdocs$docs)\n  fkread <- function (fname) {\n    ftext <- qsplitdocs$text[qsplitdocs$docs==fname]\n    flesch_kincaid (ftext)\n  }\n  scores <- lapply (perFile, fkread)\n  \n  scores\n}\n\n# can only use this function if we haven't done removeNumbers with tm_map\nextractYears <- function (docs) {\n  \n  ydict <- c(1960:2040)\n  yfunc <- function (doc) {\n    # restrict to 4 character words, to searching for values between 1960 and 2040, only return values where there's a hit\n    # some of these settings are probably made redundent by the others, but being safe\n    l <- termFreq(doc, control=list(wordLengths=c(4,4), dictionary=ydict, bounds = list(local = c(1, Inf))))\n    # get min and max of returned vals\n    ll <- c(l[1],l[length(l)])\n  }\n  \n  # do the above function for all the documents, using lapply\n yrange <- lapply (docs, yfunc)\n \n yrange\n}\n\n# this uses a termdocumentmatrix as opposed to documenttermmatrix\nwordScoresImpactWeighted <- function (docs, impScores, occurrences=c(2,Inf)) {\n  tdm <- TermDocumentMatrix(docs, control = list(bounds = list(global = occurrences), wordLengths=c(2,18), weighting = weightTfIdf))\n  tdmm <- as.matrix (tdm)\n  \n  # Weight terms that occur in institutions statement by impact score of that institute\n  ww <- tdmm %*% impScores$GPA\n  # Weight those scores by the occurence of those terms across whole corpus\n  ww2 <- ww / rowSums(tdmm)\n  \n  l <- length (ww2)\n  cat (l, \"terms\")\n  top <- ww2[order(ww2, decreasing=TRUE),][1:20]\n  bottom <- ww2[order(ww2, decreasing = TRUE), ][max(l-19, 1):l]\n  \n  c(top,bottom)\n}\n\n\n\nfilterOutNA <- function (docs, impScores) {\n  # myFilter tests whether uniname of doc gives a non-NA result in the impScores table\n  # this produces fvals - a true, false array - which is used to filter docs to make docs2\n  print (impScores)\n  print (names(docs))\n  \n  myFilter <- function (doc) { !is.na (impScores[doc$meta$name]$GPA) }\n  fvals <- lapply (docs, myFilter)\n  print (fvals)\n  filtDocs <- docs[as.logical(fvals)]\n  print (\"doc objs\")\n  print (names(filtDocs))\n  \n  filtDocs\n}\n\n\n# occurrence is a bandpass filter on the number of documents a term needs to occur in\n# to make it in to the matrix.\nmakeTermMatrices <- function (docs, occurrence = c(2,Inf)) {\n  library(tm)\n  \n  dtm <- DocumentTermMatrix(docs, control = list(bounds = list(global = occurrence), wordLengths=c(2,18)))\n  #inspect(dtm[1:12, 5001:5010])\n  \n  BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\n  #dtm <- DocumentTermMatrix(ds, control = list(tokenize = BigramTokenizer))\n  \n  ## do tfxidf\n  dtm_i <- weightTfIdf(dtm)\n  \n  list (directl=dtm, inverse=dtm_i)\n}\n\n# matrix cannot be inverse frequencies (idf)\nmakeTopicModel <- function (matrix) {\n  library (topicmodels)\n  \n  topicCount <- round(sqrt(nrow(matrix) / 2))\n  lda <- LDA (matrix, k = topicCount)\n  \n  lda\n}\n\n# matrix cannot be inverse frequencies (idf)\nmakeSupervisedTopicModel <- function (matrix, impScores) {\n  library (topicmodels)\n  library (lda)\n  \n  topicCount <- round(sqrt(nrow(matrix) / 2))\n  lda_obj <- dtm2ldaformat(matrix)\n  \n  ## Initialize the params\n  params <- sample(c(-0.001, 0.001), topicCount, replace=TRUE)\n  \n  result <- slda.em(documents = lda_obj$documents,\n                    K = topicCount,\n                    vocab = lda_obj$vocab,\n                    num.e.iterations = 5,\n                    num.m.iterations = 4,\n                    alpha = 1.0,\n                    eta = 0.1,\n                    annotations = impScores$GPA,\n                    params = params,\n                    variance = 0.25,\n                    lambda = 1.0,\n                    logistic = FALSE,\n                    regularise = FALSE,\n                    method = \"sLDA\",\n                    trace = 5L\n                    )\n  \n  # these are new documents but the old ones\n  # interesting to see if they match up to their actual scores I suppose\n  predictImpScores <- slda.predict (lda_obj$documents, \n                              result$topics, \n                              result$model, \n                              alpha = 1.0, \n                              eta = 0.1,\n                           )\n  \n  predictImpScores2 <- slda.predict.docsums (lda_obj$documents, \n                                    result$topics, \n                                    alpha = 1.0, \n                                    eta = 0.1,\n  )\n  \n  list(result = result, predict = predictImpScores, predict2 = predictImpScores2)\n}\n\n\nmakeColourScale <- function (docs, impScores) {\n  cscale <- topo.colors (100)\n  colFunc = function (doc) {\n    cscale [impScores[meta(doc)$name]$GPA * 20]\n  }\n  tcol <- sapply(docs, colFunc)\n  \n  tcol\n}\n\n\ndrawPlots <- function (docs, invMatrix, impScores, tcol)\n{\n  m <- as.matrix (invMatrix)\n  rownames(m) <- 1:nrow(m)\n  \n  ### don't forget to normalize the vectors so Euclidean makes sense\n  norm_eucl <- function(m) m/apply(m, MARGIN=1, FUN=function(x) sum(x^2)^.5)\n  m_norm <- norm_eucl(m)\n  \n\n  par(mfrow=c(1, 3))\n  \n  drawKMeans (m_norm, tcol, docs, impScores)\n  drawMDS (m_norm, tcol)\n  \n  \n  terms <- c(\"ceo\", \"director\", \"msp\", \"minister\", \"president\", \"councillor\")\n  tws <- c(4, 2, 2, 5, 7, 0.5)\n  wordScores <- calcWeightedScore (docs, terms, tws)\n  \n  drawWeightGPAPlot (docs, impScores, wordScores, tcol)\n  \n  cl\n}\n\ndrawKMeans <- function (normMatrix, colScale, docs, impScores) {\n  ### cluster into 10 clusters\n  ## rool ov fumb\n  ## http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#Rule_of_thumb\n  k <- round(sqrt(nrow(normMatrix) / 2))\n  cl <- kmeans(normMatrix, k)\n  \n  tlabel <- sapply(1:length(docs), function(i) meta(docs[[i]])$name)\n  x <- cl$cluster;\n  y <- sapply (docs, function(doc) impScores[meta(doc)$name]$GPA)\n  symbols(x, y, circles = rep(0.1,length(x)), inches = FALSE, main=\"K MEANS\", xlab=\"Cluster Index\", ylab=\"Impact GPA\", bg=colScale, fg=\"white\")\n  text(x, y, labels = row.names(normMatrix), pos = 2, cex=.7, col = \"#44444480\")  \n}\n\n\n### show clusters using the first 2 principal components\ndrawMDS <- function (normMatrix, colScale) {\n  d <- dist(normMatrix) # euclidean distances between the rows\n  fit <- cmdscale(d,eig=TRUE, k=2) # k is the number of dim\n  \n  # plot solution \n  x <- fit$points[,1]\n  y <- fit$points[,2]\n  # tcol <- sapply(1:corpusSize, colFunc)\n  symbols(x, y, circles = rep(0.01,length(x)), inches = FALSE, main=\"METRIC MDS\", xlab=\"D1\", ylab=\"D2\", bg=colScale, fg=\"white\")\n  text(x, y, labels = row.names(normMatrix), pos = 2, cex=.7, col = \"#44444480\")\n}\n\ncalcWeightedScore <- function (docs, terms, weights) {\n  print (length(weights))\n  \n  perDoc <- function (doc) {\n    freq <- termFreq (doc, control = list(dictionary=terms))\n    #print (length(freq))\n    #print (freq)\n    wfreq <- freq * weights\n    wsum <- sum (wfreq)\n    wsum\n  }\n  wscores <- lapply (docs, perDoc)\n  \n  wscores\n}\n\n\ndrawWeightGPAPlot <- function (docs, impScores, weightScores, colScale) {\n  x <- weightScores\n  y <- sapply (docs, function(doc) impScores[meta(doc)$name]$GPA)\n  fixr <- (max(weightScores) - min(weightScores) + 1) / 200; # resolution of 200\n  symbols(x, y, circles = rep (fixr, length(x)), inches = FALSE, main=\"WEIGHT / GPA\", xlab=\"WEIGHT\", ylab=\"Impact GPA\", bg=colScale, fg=\"white\")\n  text(x, y, labels = c(1:length(docs)), pos = 2, cex=.7, col = \"#44444480\")\n}\n\n",
    "created" : 1432817520578.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1902491153",
    "id" : "B94A2E52",
    "lastKnownWriteTime" : 1433420464,
    "path" : "C:/Users/cs22/workspace/refimpact/R/textAnalysis.r",
    "project_path" : "R/textAnalysis.r",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}